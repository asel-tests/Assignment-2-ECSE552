{"cells":[{"cell_type":"code","execution_count":null,"id":"31f90f36-4fc2-4a5b-a403-5049d778a21e","metadata":{"id":"31f90f36-4fc2-4a5b-a403-5049d778a21e"},"outputs":[],"source":["!pip install pip install umap-learn\n","!pip install pytorch-lightning\n","import tarfile\n","from PIL import Image\n","from glob import glob\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import datasets, transforms\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","import umap\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"id":"661e7330-981a-4480-b4d9-70f1cde45d9b","metadata":{"id":"661e7330-981a-4480-b4d9-70f1cde45d9b"},"outputs":[],"source":["import matplotlib\n","font = {'weight' : 'regular',\n","        'size'   : 22}\n","\n","matplotlib.rc('font', **font)"]},{"cell_type":"code","execution_count":null,"id":"80a4610b-a925-4520-a5cb-84bc8c8b53c1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80a4610b-a925-4520-a5cb-84bc8c8b53c1","outputId":"9057d6f2-5573-4c9c-ba98-f909a9d6e957"},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  1610  100  1610    0     0   5047      0 --:--:-- --:--:-- --:--:--  5031\n"]}],"source":["!curl -O https://gist.githubusercontent.com/jszym/479db2af32411b64249bfb1bff43a95e/raw/1bef32b78e6ddd414beebe6e57cf3b0a0fac44ea/dictlogger.py\n","from dictlogger import DictLogger"]},{"cell_type":"markdown","id":"b5ec8f86-9975-418a-9e54-65db797dc8d0","metadata":{"id":"b5ec8f86-9975-418a-9e54-65db797dc8d0"},"source":["# ECSE552 - W25 - Assignment 2 - Part 2 (Total 20 points)"]},{"cell_type":"markdown","id":"e77b5d54-1b01-49e0-9a44-c59cccb23de0","metadata":{"id":"e77b5d54-1b01-49e0-9a44-c59cccb23de0"},"source":["## Conditional Variational Autoencoders\n","\n","### Note\n","\n","***Clearly comment each of your steps. Failure to do so would result in loss of points.***\n","**Additionally explain each line in your code**\n","\n","### Introduction\n","\n","Conditional Variational Autoencoders (CVAEs) are an extension of Variational Autoencoders (VAEs).\n","\n","As we explored in tutorial eight, VAEs are comprised of two main parts: an encoder and a decoder.\n","\n","The encoder approximates the function $Q(z|X)$ and the decoder approximates the function $P(X|z)$.\n","\n","As a reminder, here $X$ is the input data and $z$ is a latent vector.\n","\n","For more details, you might consider this [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) by Carl Doersch.\n","\n","The above formulation, however, does not allow us to generate specific images. For example, given a model trained on the MNIST digits, it's difficult to fully explore the latent space for a given digit.\n","\n","We can solve this by conditioning $P$ by not only the latent vector $z$, but some value $c$ that can encode information, (_e.g._: which MNIST digit we're generating).\n","\n","This would mean our decoder would got from $P(X|z)$ in the VAE case to $P(X|z,c)$ in the CVAE case.\n","\n","### Practical Implementation\n","\n","Practically, this means that a \"conditional\" vector $c$ is passed to the decoder. This is commonly a class label for the sample that is one-hot encoded, but it can be anything you wish to marginalise the model by.\n","\n","Below is a very high-level illustration of both a Variational Autoencoder (VAE) and a Conditional Variational Autoencoder (CVAE). Components of the CVAE that are not present in the VAE are demarked with a yellow star.\n","\n","The CVAE is identical to the VAE, but for the concatenation of the vector $c$ to the vector $z$ before being inputted into the decoder. The loss function does not change in any way.\n","\n","In this diagram the dimensions are noted for all the inputs and outputs of the Encoder and Decoder.\n","\n","The dimension variables are as follows:\n","\n","* $m$ is the batch size\n","* $w$ is the width of the image inputted into our encoder\n","* $h$ is the height of the image inputted into our encoder\n","* $n$ is the number of latent variables\n","* $q$ is the width of the conditional vector. If $c$ represents the class of the image, then $q$ is often equal to the number of classes.\n","\n","<img src=\"https://ecse552.b-cdn.net/W24/H3/vae_cvae.webp\" alt=\"VAE vs CVAE\" width=\"50%\"/>\n","\n","Let's look closer at the concatenation of $z$ and $c$, as well as the representation of $c$. Consider the case of a CVAE where the samples are MNIST digits and $c$ is the one-hot encoded class of the digit. In this example, the number of latent variables ($n$ in the diagram above) is 3. Below is an illustration of what the concatenation might look like.\n","\n","![Concatenating z with c](https://ecse552.b-cdn.net/W24/H3/conditional_z.webp)\n","\n","The vectors that are the result of concatenating $z$ and $c$ (let's call it $z_c$) then become the input to the decoder."]},{"cell_type":"markdown","id":"129b729e-c5c3-4eda-8fb8-09dc9882505c","metadata":{"id":"129b729e-c5c3-4eda-8fb8-09dc9882505c"},"source":["### Q1.1 - Implementing an MNIST CVAE (10 points)\n","\n","Your task is to extend the MNIST VAE from Tutorial 8 to a CVAE.\n","\n","Specifically, you must:\n","\n","1. Correctly define a conditional vector $c$ in the training loop.\n","2. Correctly concatenate it to $z$ to form $z_c$.\n","3. Input $z_c$ to the Decoder.\n","4. Make any further nescessary modifications to the code.\n","\n","You are provided with the complete code for the MNIST VAE presented in Tutorial 8, including data loaders, encoders, decoders, and training/validation loops.\n","\n","The number of latent variables for this excercise ($n$) is set to 10.\n","\n","**Hint**: While you are permitted to make changes to the Encoder and Decoder class, it is not nescessary to correctly answer this question.\n","\n","**Another Hint**: You can correctly answer this question by modifying/adding no more than six lines."]},{"cell_type":"code","execution_count":null,"id":"3b7dff07-e634-4c4a-8abe-5438bd2de14d","metadata":{"id":"3b7dff07-e634-4c4a-8abe-5438bd2de14d"},"outputs":[],"source":["class MNISTDataset(Dataset):\n","\n","    def __init__(self, csv_path):\n","\n","        self.df = pd.read_csv(csv_path)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","\n","        return torch.tensor(self.df.iloc[idx][1:])/255, self.df.iloc[idx][0]"]},{"cell_type":"code","execution_count":null,"id":"48e82a9e-2373-41ef-866d-9c22f1e0c338","metadata":{"id":"48e82a9e-2373-41ef-866d-9c22f1e0c338"},"outputs":[],"source":["batch_size = 100\n","\n","dataset = MNISTDataset('./sample_data/mnist_train_small.csv')\n","\n","num_test = len(dataset) // 10\n","num_train = len(dataset) - num_test\n","dataset_train, dataset_test = random_split(dataset, [num_train, num_test])\n","\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"id":"6e022cf9-7029-4fa3-8d0b-85132cedcbbe","metadata":{"id":"6e022cf9-7029-4fa3-8d0b-85132cedcbbe"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, in_dim, latent_dim):\n","        super(Encoder, self).__init__()\n","\n","        self.layer0 = nn.Linear(in_dim, 1024)\n","        self.layer1 = nn.Linear(512, 768)\n","        self.layer2 = nn.Linear(768, 1024)\n","        self.layer3 = nn.Linear(512, 256)\n","        self.layer4 = nn.Linear(128, latent_dim)\n","\n","        self.pool = nn.MaxPool1d(2)\n","\n","        self.dropout = nn.Dropout(0.3)\n","\n","        self.activation = nn.LeakyReLU(0.01)\n","\n","    def forward(self, x):\n","\n","        x = self.layer0(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = x.reshape(x.shape[0], 1, x.shape[1])\n","        x = self.pool(x)\n","        x = x.reshape(x.shape[0], x.shape[2])\n","\n","        x = self.layer1(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer2(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = x.reshape(x.shape[0], 1, x.shape[1])\n","        x = self.pool(x)\n","        x = x.reshape(x.shape[0], x.shape[2])\n","\n","        x = self.layer3(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = x.reshape(x.shape[0], 1, x.shape[1])\n","        x = self.pool(x)\n","        x = x.reshape(x.shape[0], x.shape[2])\n","\n","        x = self.layer4(x)\n","        x = self.activation(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"a9980357-db64-4e8f-9e6e-26dff27e6155","metadata":{"id":"a9980357-db64-4e8f-9e6e-26dff27e6155"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dim, out_dim):\n","        super(Decoder, self).__init__()\n","\n","        self.layer4 = nn.Linear(784, out_dim)\n","        self.layer3 = nn.Linear(768, 784)\n","        self.layer2 = nn.Linear(1024, 768)\n","        self.layer1 = nn.Linear(256, 1024)\n","        self.layer0 = nn.Linear(latent_dim, 256)\n","\n","        self.dropout = nn.Dropout(0.3)\n","\n","        self.activation = nn.LeakyReLU(0.01)\n","\n","    def forward(self, x):\n","\n","        x = self.layer0(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer1(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer2(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer3(x)\n","        x = self.activation(x)\n","\n","        x = self.dropout(x)\n","\n","        x = self.layer4(x)\n","        x = torch.sigmoid(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"126f99c3-ba20-40ec-8872-77f58a90447b","metadata":{"id":"126f99c3-ba20-40ec-8872-77f58a90447b"},"outputs":[],"source":["class MNIST_VAE(pl.LightningModule):\n","    def __init__(self, in_dim, out_dim, latent_dim):\n","        super(MNIST_VAE, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","\n","        self.encoder = Encoder(in_dim, latent_dim*2)\n","        self.decoder = Decoder(latent_dim, out_dim)\n","        self.recon_criterion = nn.BCELoss(reduction='sum')\n","\n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu)\n","\n","    def loss(self, mu, logvar, pred, target):\n","\n","        kld_loss = torch.sum(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n","        recons_loss = self.recon_criterion(pred, target)\n","\n","        loss = recons_loss + kld_loss\n","\n","        return kld_loss, recons_loss, loss\n","\n","    def forward(self, x):\n","\n","        parameters= self.encoder(x)\n","\n","        mu = parameters[:, :self.latent_dim]\n","        logvar = parameters[:, self.latent_dim:]\n","\n","        z = self.reparameterize(mu, logvar)\n","\n","        x_hat = self.decoder(z)\n","\n","        return mu, logvar, z, x_hat\n","\n","    def training_step(self, batch, batch_idx):\n","\n","        x, y = batch\n","\n","        mu, logvar, z, x_hat = self(x)\n","\n","        kld_loss, recons_loss, loss = self.loss(mu, logvar, x_hat, x)\n","\n","        self.log('training_loss', loss, on_step=False, on_epoch=True)\n","        self.log('training_kld_loss', kld_loss, on_step=False, on_epoch=True)\n","        self.log('training_recons_loss', recons_loss, on_step=False, on_epoch=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","\n","        x, y = batch\n","\n","        mu, logvar, z, x_hat = self(x)\n","\n","        kld_loss, recons_loss, loss = self.loss(mu, logvar, x_hat, x)\n","\n","        self.log('val_loss', loss, on_step=False, on_epoch=True)\n","        self.log('val_kld_loss', kld_loss, on_step=False, on_epoch=True)\n","        self.log('val_recons_loss', recons_loss, on_step=False, on_epoch=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n","        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(dataloader_train), epochs=20)\n","        return [optimizer], [scheduler]"]},{"cell_type":"code","execution_count":null,"id":"3cdb208d-c705-433e-b605-708b4f235adc","metadata":{"id":"3cdb208d-c705-433e-b605-708b4f235adc"},"outputs":[],"source":["model = MNIST_VAE(784, 784, 10)"]},{"cell_type":"code","execution_count":null,"id":"6f34edcb-4812-46ca-8483-9dd5052aa4d0","metadata":{"id":"6f34edcb-4812-46ca-8483-9dd5052aa4d0"},"outputs":[],"source":["logger = DictLogger()\n","early_stopping = EarlyStopping('val_loss', verbose=True, min_delta=0.0001,\n","                               patience=3)\n","trainer = pl.Trainer(gpus=1, logger=logger, callbacks=[early_stopping],\n","                     max_epochs=40)\n","\n","\n","trainer.fit(model, dataloader_train, dataloader_test)"]},{"cell_type":"code","execution_count":null,"id":"e8bdaa2c-d727-42dc-8c81-0508b7f49591","metadata":{"id":"e8bdaa2c-d727-42dc-8c81-0508b7f49591"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","plt.plot(logger.metrics['training_loss'], label='Training', lw=3)\n","plt.plot(logger.metrics['val_loss'], label='Val', lw=3)\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"65bea93e-5567-4b61-94fa-7060f2a4f265","metadata":{"id":"65bea93e-5567-4b61-94fa-7060f2a4f265"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","plt.plot(logger.metrics['training_kld_loss'], label='KLD', lw=3)\n","plt.plot(logger.metrics['training_recons_loss'], label='Recon', lw=3)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"ee6f65b5-45ae-49b5-93f2-012646b8051d","metadata":{"id":"ee6f65b5-45ae-49b5-93f2-012646b8051d"},"source":["### Q1.2 Demonstrate Reconstruction (5 points)\n","\n","In the same way as was demonstrated in Tutorial 8, the ability of your model to reconstruct the first two digits from the first batch provided by `dataloader_test`.\n","\n","Specifically, pass the first two values of `x` and `y` from `dataloader_test` to `model` and reconstruct these images. Plot them as illustrated below:\n","\n","<img src=https://ecse552.b-cdn.net/W24/H3/q1.2.webp alt=\"Reconstructed MNIST digits\" width=\"35%\"/>\n","\n","**Note**: The batches are randomised so you won't nescessarily get the same samples shown above."]},{"cell_type":"code","execution_count":null,"id":"04ec42b6-2f6e-40dc-8e4c-1069d42e25ca","metadata":{"id":"04ec42b6-2f6e-40dc-8e4c-1069d42e25ca"},"outputs":[],"source":["# Q1.2 Code Here"]},{"cell_type":"markdown","id":"8632685c-46df-4553-b476-f8bcd7f4d959","metadata":{"id":"8632685c-46df-4553-b476-f8bcd7f4d959"},"source":["### Q1.3 Demonstrate Conditional Reconstruction (5 points)\n","\n","Similar to Q1.2, reconstruct digits, but instead of using the correct $c$ vector, change the class label to a different one.\n","\n","For example, the value of `y` for the two samples below are 3 and 6, but when reconstructing, the $c$ vector `[0,0,1,0,0,0,0,0,0,0]` was supplied.\n","\n","<img src=\"https://ecse552.b-cdn.net/W24/H3/q1.3.png\" alt=\"Conditioned Reconstruction\" width=\"35%\"/>"]},{"cell_type":"code","execution_count":null,"id":"e5202413-1656-4a97-b36d-b123f212e7b2","metadata":{"id":"e5202413-1656-4a97-b36d-b123f212e7b2"},"outputs":[],"source":["# Q1.3"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}