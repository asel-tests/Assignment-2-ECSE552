{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ixUVgOcvNh"
      },
      "source": [
        "# Assignment 2 - Part 1 (40 points)\n",
        "\n",
        "In part 1, you need to implement some forward and backward passes for some builing blocks studied during the course.\n",
        "\n",
        "**Note:** When implementing forward and backward passes, you must provide a detailed derivation of the functions and the derivatives, respectively.\n",
        "\n",
        "## **Where specified, include computational graphs**.\n",
        "Refer to the example presented during the tutorial (see uploaded image under Tutorial 6 on MyCourses).\n",
        "\n",
        "\n",
        "## **Where indicated, clearly explain your equations along with step-by-step justifications**.\n",
        "\n",
        "You may include them directly in your submitted notebook or as a separate PDF file. If the latter, indicate which page your answer corresponds to directly in your notebook (e.g., \"See page X\").\n",
        "In your attached solutions:\n",
        "- Clearly title each section to its corresponding section (e.g., \"Part 1 - Batch norm backward\", etc.).\n",
        "- Be sure to include dimensions for your variables and operations.\n",
        "- Include unique equation numbers.\n",
        "  - We suggest grouping your equations by problem (e.g., for BatchNorm backwards: (Eqn 1.0, ... 1.n); for CNN backwards: (Eqn 2.0, ..., 2.n), etc.). This is not mandatory, as long as your work consistent and clear. See example below.\n",
        "- You may submit your writen solutions in a single called `solutions.pdf` with the rest of your submission `.zip` file.\n",
        "- You may handwrite it on paper and scan to a PDF, but make sure your writing is *legible* and workflow is easy to follow.\n",
        "- Include page numbers.\n",
        "\n",
        "## For e.g. (from assignment 1):\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "### Part 1 - backpropagation\n",
        "We compute the gradient of the weights using:\n",
        "\n",
        "(Eqn 1.0)\n",
        "\\begin{equation}\n",
        "  \\frac{\\partial J}{dw} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial w}\n",
        "\\end{equation}\n",
        "where,\n",
        "\n",
        "(Eqn 1.1)  \n",
        "\\begin{equation}\n",
        "  \\frac{\\partial J}{\\partial a} = \\frac{\\partial L}{\\partial a} = -[y \\log(a) + (1-y)\\log(1-a)]\\frac{\\partial}{\\partial a} = -[y\\frac{1}{a}+(1-y)\\frac{1}{1-a}(-1)]\n",
        "\\end{equation}\n",
        "with dimension $(1,m)$.\n",
        "\n",
        "$\\vdots$\n",
        "\n",
        "Putting together (Eqn 1.\\_) and (Eqn 1.\\_) ...:\n",
        "\n",
        "(Eqn 1.last)\n",
        "\\begin{equation}\n",
        "  \\frac{\\partial J}{\\partial w} = (A-Y)X\n",
        "\\end{equation}\n",
        "Since $A$ has shape $(1,m)$, $Y$ has shape $(1,m)$, and $X$ is size $(n, m)$; where $n=n_{features}=(n_{pixHeight} * n_{pixWidth} * n_{channels})$\n",
        "\n",
        "$m=n_{samples}$\n",
        "\n",
        "We take the transpose of $(A-Y)$ to obtain\n",
        "$\\frac{\\partial J}{\\partial w}$ with dimensions: $(n,1) = (n,m)(m,1)$\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "## **Include (brief) comments to explain each line in your code directly**.\n",
        "For e.g.,\n",
        "```\n",
        "# add weights and bias to input sample\n",
        "# w.T: (n, 1) -> (1,n)\n",
        "z = np.dot(w.T, X) + b # (1,n)(n,m)+(1,m) = (1,m)\n",
        "\n",
        "```\n",
        "OR you may refer to the equation number from your written solutions, for e.g.:\n",
        "```\n",
        "# Eqn (1._) page N\n",
        "z = np.dot(w.T, X) + b # (1,m)\n",
        "\n",
        "```\n",
        "\n",
        "It is sufficient to simply show the change of dimensions. For e.g., if you are applying `torch.unsqueeze(z,0)` it suffices to simply comment `# (1,m) -> (1,1,m)` or `# add single dimension at index 0`.\n",
        "\n",
        "Include comments for each step, even if it is obvious. For e.g.,\n",
        "```\n",
        "# calculate BCE loss\n",
        "loss = criterion(y_pred, y)\n",
        "```\n",
        "\n",
        "## Failure to meet these requirements will result in **zero marks**.\n",
        "\n",
        "\n",
        "Read each question carefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8vs3FJ7ZcvNk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# As usual, a bit of setup\n",
        "\n",
        "import numpy as np\n",
        "from ECSE552.gradient_check import eval_numerical_gradient_array\n",
        "from ECSE552.layers import *\n",
        "from ECSE552.rnn_layers import *\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "  \"\"\" Computes the relative error between two arrays ùë• and ùë¶, which is a measure of how different they are in a normalized way. \"\"\"\n",
        "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYNg-FgNcvNl"
      },
      "source": [
        "# Batch Normalization (10 points)\n",
        "Batch normalization is a technique that improves deep network training by normalizing activations within the network. While input data can be preprocessed to have zero mean and unit variance, activations in deeper layers shift as training progresses, making optimization harder. Batch normalization addresses this by normalizing features within each minibatch using estimated means and standard deviations, reducing internal covariate shift. During training, a running average of these statistics is maintained for use during inference. To preserve representational power, learnable shift and scale parameters allow the network to adjust normalized features as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSKfnSE0cvNm"
      },
      "source": [
        "## Batch normalization: Forward (3 points)\n",
        "In the file `ECSE552/layers.py`, implement the batch normalization forward pass in the function `batchnorm_forward`. Once you have done so, run the following to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "8j9ft2MOcvNm",
        "outputId": "dd554d37-f7f2-4f0a-e9be-baf7a9842948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before batch normalization:\n",
            "  means:  [  0.94616008 -17.20223184  20.39298483]\n",
            "  stds:  [29.64139051 35.85181985 33.21075068]\n",
            "After batch normalization (gamma=1, beta=0)\n",
            "  mean:  [ 4.66293670e-17  3.71924713e-17 -2.64233080e-16]\n",
            "  std:  [0.99999999 1.         1.        ]\n",
            "After batch normalization (nontrivial gamma, beta)\n",
            "  means:  [11. 12. 13.]\n",
            "  stds:  [0.99999999 1.99999999 2.99999999]\n"
          ]
        }
      ],
      "source": [
        "# Check the training-time forward pass by checking means and variances\n",
        "# of features both before and after batch normalization\n",
        "\n",
        "# Simulate the forward pass for a two-layer network\n",
        "N, D1, D2, D3 = 200, 50, 60, 3\n",
        "X = np.random.randn(N, D1)\n",
        "W1 = np.random.randn(D1, D2)\n",
        "W2 = np.random.randn(D2, D3)\n",
        "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
        "\n",
        "print ('Before batch normalization:')\n",
        "print ('  means: ', a.mean(axis=0))\n",
        "print ('  stds: ', a.std(axis=0))\n",
        "\n",
        "# Means should be close to zero and stds close to one\n",
        "print ('After batch normalization (gamma=1, beta=0)')\n",
        "a_norm, _ = batchnorm_forward(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n",
        "print ('  mean: ', a_norm.mean(axis=0))\n",
        "print ('  std: ', a_norm.std(axis=0))\n",
        "\n",
        "# Now means should be close to beta and stds close to gamma\n",
        "gamma = np.asarray([1.0, 2.0, 3.0])\n",
        "beta = np.asarray([11.0, 12.0, 13.0])\n",
        "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
        "print ('After batch normalization (nontrivial gamma, beta)')\n",
        "print ('  means: ', a_norm.mean(axis=0))\n",
        "print ('  stds: ', a_norm.std(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "LNG48HiRcvNm",
        "outputId": "e47ae1d9-874d-4f77-b562-b98c2480c600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After batch normalization (test-time):\n",
            "  means:  [-0.08606969 -0.05940733 -0.0006442 ]\n",
            "  stds:  [1.01777947 1.04960768 0.99608292]\n"
          ]
        }
      ],
      "source": [
        "# Check the test-time forward pass by running the training-time\n",
        "# forward pass many times to warm up the running averages, and then\n",
        "# checking the means and variances of activations after a test-time\n",
        "# forward pass.\n",
        "\n",
        "N, D1, D2, D3 = 200, 50, 60, 3\n",
        "W1 = np.random.randn(D1, D2)\n",
        "W2 = np.random.randn(D2, D3)\n",
        "\n",
        "bn_param = {'mode': 'train'}\n",
        "gamma = np.ones(D3)\n",
        "beta = np.zeros(D3)\n",
        "for t in range(50):\n",
        "  X = np.random.randn(N, D1)\n",
        "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
        "  batchnorm_forward(a, gamma, beta, bn_param)\n",
        "bn_param['mode'] = 'test'\n",
        "X = np.random.randn(N, D1)\n",
        "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
        "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
        "\n",
        "# Means should be close to zero and stds close to one, but will be\n",
        "# noisier than training-time forward passes.\n",
        "print ('After batch normalization (test-time):')\n",
        "print ('  means: ', a_norm.mean(axis=0))\n",
        "print ('  stds: ', a_norm.std(axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyr2AgTwcvNn"
      },
      "source": [
        "## Batch Normalization: backward (7 points)\n",
        "Now implement the backward pass for batch normalization in the function `batchnorm_backward`.\n",
        "\n",
        "To derive the backward pass you should write out the computation graph for batch normalization and backprop through each of the intermediate nodes. Some intermediates may have multiple outgoing branches; make sure to sum gradients across these branches in the backward pass.\n",
        "\n",
        "For this function you need to show the computational graph and explain each step of how you computed the gradient.\n",
        "\n",
        "Once you have finished, run the following to numerically check your backward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYgkq5encvNn"
      },
      "source": [
        "Computational Graph: ##### ATTACH IMAGE OF COMPUTATIONAL GRAPH (either directly in markdown or as PDF included in submitted .zip) ######\n",
        "\n",
        "\n",
        "**Explaination with equations:** ##### ADD YOU ANSWER DIRECTLY HERE ######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTk_Qd3IcvNn",
        "outputId": "ac31a1aa-51d5-48ac-ebc1-52e671061c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dx error:  9.076502945910793e-09\n",
            "dgamma error:  3.821013801928778e-12\n",
            "dbeta error:  4.282006285483895e-12\n"
          ]
        }
      ],
      "source": [
        "# Gradient check batchnorm backward pass\n",
        "\n",
        "N, D = 4, 5\n",
        "x = 5 * np.random.randn(N, D) + 12\n",
        "gamma = np.random.randn(D)\n",
        "beta = np.random.randn(D)\n",
        "dout = np.random.randn(N, D)\n",
        "\n",
        "bn_param = {'mode': 'train'}\n",
        "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
        "fg = lambda a: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
        "fb = lambda b: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
        "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
        "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
        "\n",
        "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
        "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
        "print ('dx error: ', rel_error(dx_num, dx))\n",
        "print ('dgamma error: ', rel_error(da_num, dgamma))\n",
        "print ('dbeta error: ', rel_error(db_num, dbeta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QllHeXT6cvNn"
      },
      "source": [
        "# Convolutional Neural Networks (10 points)\n",
        "So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead for images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSspuvAhcvNn"
      },
      "source": [
        "## Convolution: Naive forward pass (2 points)\n",
        "The core of a convolutional network is the convolution operation. In the file `ECSE552/layers.py`, implement the forward pass for the convolution layer in the function `conv_forward_naive`.\n",
        "\n",
        "You don't have to worry too much about efficiency at this point; just write the code in whatever way you find most clear.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "You can test your implementation by running the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0ixhBEDcvNo",
        "outputId": "0260d1f8-b4e8-44a5-8bc1-d67f10086d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing conv_forward_naive\n",
            "difference:  2.2121476417505994e-08\n"
          ]
        }
      ],
      "source": [
        "x_shape = (2, 3, 4, 4)\n",
        "w_shape = (3, 3, 4, 4)\n",
        "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
        "b = np.linspace(-0.1, 0.2, num=3)\n",
        "\n",
        "conv_param = {'stride': 2, 'pad': 1}\n",
        "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
        "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
        "                           [-0.18387192, -0.2109216 ]],\n",
        "                          [[ 0.21027089,  0.21661097],\n",
        "                           [ 0.22847626,  0.23004637]],\n",
        "                          [[ 0.50813986,  0.54309974],\n",
        "                           [ 0.64082444,  0.67101435]]],\n",
        "                         [[[-0.98053589, -1.03143541],\n",
        "                           [-1.19128892, -1.24695841]],\n",
        "                          [[ 0.69108355,  0.66880383],\n",
        "                           [ 0.59480972,  0.56776003]],\n",
        "                          [[ 2.36270298,  2.36904306],\n",
        "                           [ 2.38090835,  2.38247847]]]]])\n",
        "\n",
        "# Compare your output to ours; difference should be around 1e-8\n",
        "print ('Testing conv_forward_naive')\n",
        "print ('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5edkwnz7cvNo"
      },
      "source": [
        "## Convolution: Naive backward pass (3 points)\n",
        "Implement the backward pass for the convolution operation in the function `conv_backward_naive` in the file `ECSE552/layers.py`. Again, you don't need to worry too much about computational efficiency.\n",
        "\n",
        "Explain in words step by step how you came up the derivatives with equations. Computational graph is recommened but not required.\n",
        "\n",
        "When you are done, run the following to check your backward pass with a numeric gradient check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkLMWxI8cvNo"
      },
      "source": [
        "**Explaination with equations:** ##### ADD YOU ANSWER DIRECTLY HERE ######\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UBOjPeNcvNo",
        "outputId": "e99ffd30-805a-4cd6-f4db-54b52c4fe310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing conv_backward_naive function\n",
            "dx error:  3.255383838449905e-09\n",
            "dw error:  5.099903024661532e-10\n",
            "db error:  1.454655682789745e-09\n"
          ]
        }
      ],
      "source": [
        "x = np.random.randn(4, 3, 5, 5)\n",
        "w = np.random.randn(2, 3, 3, 3)\n",
        "b = np.random.randn(2,)\n",
        "dout = np.random.randn(4, 2, 5, 5)\n",
        "conv_param = {'stride': 1, 'pad': 1}\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
        "\n",
        "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
        "dx, dw, db = conv_backward_naive(dout, cache)\n",
        "\n",
        "# Your errors should be around 1e-9'\n",
        "print ('Testing conv_backward_naive function')\n",
        "print ('dx error: ', rel_error(dx, dx_num))\n",
        "print ('dw error: ', rel_error(dw, dw_num))\n",
        "print ('db error: ', rel_error(db, db_num))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p6ngGMwcvNo"
      },
      "source": [
        "## Max pooling: Naive forward (2 points)\n",
        "Implement the forward pass for the max-pooling operation in the function `max_pool_forward_naive` in the file `ECSE552/layers.py`. Again, don't worry too much about computational efficiency.\n",
        "\n",
        "Check your implementation by running the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuYjsFW6cvNo",
        "outputId": "e3a82176-6f1f-4a8e-eec2-3d1513b50c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing max_pool_forward_naive function:\n",
            "difference:  4.1666665157267834e-08\n"
          ]
        }
      ],
      "source": [
        "x_shape = (2, 3, 4, 4)\n",
        "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
        "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
        "\n",
        "out, _ = max_pool_forward_naive(x, pool_param)\n",
        "\n",
        "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
        "                          [-0.20421053, -0.18947368]],\n",
        "                         [[-0.14526316, -0.13052632],\n",
        "                          [-0.08631579, -0.07157895]],\n",
        "                         [[-0.02736842, -0.01263158],\n",
        "                          [ 0.03157895,  0.04631579]]],\n",
        "                        [[[ 0.09052632,  0.10526316],\n",
        "                          [ 0.14947368,  0.16421053]],\n",
        "                         [[ 0.20842105,  0.22315789],\n",
        "                          [ 0.26736842,  0.28210526]],\n",
        "                         [[ 0.32631579,  0.34105263],\n",
        "                          [ 0.38526316,  0.4       ]]]])\n",
        "\n",
        "# Compare your output with ours. Difference should be around 1e-8.\n",
        "print ('Testing max_pool_forward_naive function:')\n",
        "print ('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbQVymy9cvNp"
      },
      "source": [
        "# Max pooling: Naive backward (3 points)\n",
        "Implement the backward pass for the max-pooling operation in the function `max_pool_backward_naive` in the file `ECSE552/layers.py`. You don't need to worry about computational efficiency.\n",
        "\n",
        "Explain in words step by step how you came up the derivatives. (Add equations). Computational graph is recommened but not required.\n",
        "\n",
        "Check your implementation with numeric gradient checking by running the following"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkp7_y5ecvNp"
      },
      "source": [
        "**Explaination with equations:** ##### ADD YOU ANSWER DIRECTLY HERE ######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO7ZpVq7cvNp",
        "outputId": "a08665c8-1324-49bb-f787-8fb241576578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing max_pool_backward_naive function:\n",
            "dx error:  3.275616721348204e-12\n"
          ]
        }
      ],
      "source": [
        "x = np.random.randn(3, 2, 8, 8)\n",
        "dout = np.random.randn(3, 2, 4, 4)\n",
        "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
        "\n",
        "out, cache = max_pool_forward_naive(x, pool_param)\n",
        "dx = max_pool_backward_naive(dout, cache)\n",
        "\n",
        "# Your error should be around 1e-12\n",
        "print ('Testing max_pool_backward_naive function:')\n",
        "print ('dx error: ', rel_error(dx, dx_num))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb4lJWDzcvNp"
      },
      "source": [
        "# Recurrent Neural Networks (10 points)\n",
        "\n",
        "In this task we would implement the forward and backward pass different types of RNN layers in `ECSE552/rnn_layers.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdHHQ97CcvNp"
      },
      "source": [
        "## Vanilla RNN: step forward (2 points)\n",
        "Open the file `ECSE552/rnn_layers.py`. This file implements the forward and backward passes for different types of layers that are commonly used in recurrent neural networks.\n",
        "\n",
        "First implement the function `rnn_step_forward` which implements the forward pass for a single timestep of a vanilla recurrent neural network. After doing so run the following to check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgAFyg3ycvNp",
        "outputId": "02397216-8434-45b3-902f-1c0faded0ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "next_h error:  6.292421426471037e-09\n"
          ]
        }
      ],
      "source": [
        "N, D, H = 3, 10, 4\n",
        "\n",
        "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
        "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
        "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
        "b = np.linspace(-0.2, 0.4, num=H)\n",
        "\n",
        "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
        "expected_next_h = np.asarray([\n",
        "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
        "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
        "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
        "\n",
        "print ('next_h error: ', rel_error(expected_next_h, next_h))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_l9gKDmcvNp"
      },
      "source": [
        "## Vanilla RNN: step backward (3 points)\n",
        "In the file `ECSE552/rnn_layers.py` implement the `rnn_step_backward` function. After doing so run the following to numerically gradient check your implementation. You should see errors less than `1e-8`.\n",
        "\n",
        "For this function you need to show the computational graph and explain each step of how you computed the gradient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCWEdZV7cvNp"
      },
      "source": [
        "Computational Graph: ##### ATTACH IMAGE OF COMPUTATIONAL GRAPH (either directly in markdown or as PDF included in submitted .zip) ######\n",
        "\n",
        "\n",
        "**Explaination with equations:** ##### ADD YOU ANSWER DIRECTLY HERE ######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qg_EVIEcvNp",
        "outputId": "e18665c2-e6c9-40aa-f99c-dc4fb9e4be1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dx error:  1.4237722874236568e-10\n",
            "dprev_h error:  1.8755387107565935e-10\n",
            "dWx error:  2.0931270244954666e-09\n",
            "dWh error:  2.4816343880871477e-10\n",
            "db error:  2.327671903601325e-11\n"
          ]
        }
      ],
      "source": [
        "\n",
        "N, D, H = 4, 5, 6\n",
        "x = np.random.randn(N, D)\n",
        "h = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, H)\n",
        "Wh = np.random.randn(H, H)\n",
        "b = np.random.randn(H)\n",
        "\n",
        "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
        "\n",
        "dnext_h = np.random.randn(*out.shape)\n",
        "\n",
        "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
        "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
        "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
        "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
        "\n",
        "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
        "\n",
        "print ('dx error: ', rel_error(dx_num, dx))\n",
        "print ('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
        "print ('dWx error: ', rel_error(dWx_num, dWx))\n",
        "print ('dWh error: ', rel_error(dWh_num, dWh))\n",
        "print ('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xboLnmY7cvNq"
      },
      "source": [
        "## Vanilla RNN: forward (2 points)\n",
        "Now that you have implemented the forward and backward passes for a single timestep of a vanilla RNN, you will combine these pieces to implement a RNN that process an entire sequence of data.\n",
        "\n",
        "In the file `ECSE552/rnn_layers.py`, implement the function `rnn_forward`. This should be implemented using the `rnn_step_forward` function that you defined above. After doing so run the following to check your implementation. You should see errors less than `1e-7`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brGOIzJHcvNq",
        "outputId": "12d49a1d-7ff3-4593-a9ba-1ad17eec0d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h error:  7.728466151011529e-08\n"
          ]
        }
      ],
      "source": [
        "N, T, D, H = 2, 3, 4, 5\n",
        "\n",
        "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
        "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
        "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
        "b = np.linspace(-0.7, 0.1, num=H)\n",
        "\n",
        "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
        "expected_h = np.asarray([\n",
        "  [\n",
        "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
        "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
        "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
        "  ],\n",
        "  [\n",
        "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
        "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
        "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
        "print ('h error: ', rel_error(expected_h, h))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOaOHYescvNq"
      },
      "source": [
        "## Vanilla RNN: backward (3 points)\n",
        "In the file `cs231n/rnn_layers.py`, implement the backward pass for a vanilla RNN in the function `rnn_backward`. This should run back-propagation over the entire sequence, calling into the `rnn_step_backward` function that you defined above.\n",
        "\n",
        "\n",
        "For this function you need to show add the explaination with equations of how you calculated the gradient for the layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I-HQP_ncvNq"
      },
      "source": [
        "**Explaination with equations:** ##### ADD YOU ANSWER DIRECTLY HERE ######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVK5sZXEcvNq",
        "outputId": "fcf35120-727c-4971-e091-ee471059dc89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dx error:  6.002757226918306e-10\n",
            "dh0 error:  1.9759739588430348e-10\n",
            "dWx error:  6.379525339486093e-10\n",
            "dWh error:  7.255847310658916e-09\n",
            "db error:  7.574249891983014e-11\n"
          ]
        }
      ],
      "source": [
        "N, D, T, H = 2, 3, 10, 5\n",
        "\n",
        "x = np.random.randn(N, T, D)\n",
        "h0 = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, H)\n",
        "Wh = np.random.randn(H, H)\n",
        "b = np.random.randn(H)\n",
        "\n",
        "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
        "\n",
        "dout = np.random.randn(*out.shape)\n",
        "\n",
        "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
        "\n",
        "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
        "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
        "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
        "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
        "\n",
        "print ('dx error: ', rel_error(dx_num, dx))\n",
        "print ('dh0 error: ', rel_error(dh0_num, dh0))\n",
        "print ('dWx error: ', rel_error(dWx_num, dWx))\n",
        "print ('dWh error: ', rel_error(dWh_num, dWh))\n",
        "print ('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABUPswBXcvNq"
      },
      "source": [
        "# LSTM (10 points)\n",
        "If you read recent papers, you'll see that many people use a variant on the vanialla RNN called Long-Short Term Memory (LSTM) RNNs. Vanilla RNNs can be tough to train on long sequences due to vanishing and exploding gradiants caused by repeated matrix multiplication. LSTMs solve this problem by replacing the simple update rule of the vanilla RNN with a gating mechanism as follows.\n",
        "\n",
        "Similar to the vanilla RNN, at each timestep we receive an input $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$; the LSTM also maintains an $H$-dimensional *cell state*, so we also receive the previous cell state $c_{t-1}\\in\\mathbb{R}^H$. The learnable parameters of the LSTM are an *input-to-hidden* matrix $W_x\\in\\mathbb{R}^{4H\\times D}$, a *hidden-to-hidden* matrix $W_h\\in\\mathbb{R}^{4H\\times H}$ and a *bias vector* $b\\in\\mathbb{R}^{4H}$.\n",
        "\n",
        "At each timestep we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $g\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *block input* $g\\in\\mathbb{R}^H$ as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "i = \\sigma(a_i) \\hspace{2pc}\n",
        "f = \\sigma(a_f) \\hspace{2pc}\n",
        "o = \\sigma(a_o) \\hspace{2pc}\n",
        "g = \\tanh(a_g)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function and $\\tanh$ is the hyperbolic tangent, both applied elementwise.\n",
        "\n",
        "Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as\n",
        "\n",
        "$$\n",
        "c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc}\n",
        "h_t = o\\odot\\tanh(c_t)\n",
        "$$\n",
        "\n",
        "where $\\odot$ is the elementwise product of vectors.\n",
        "\n",
        "In the rest of the notebook we will implement the LSTM update rule and apply it to the image captioning task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViPhsDoEcvNq"
      },
      "source": [
        "## LSTM: step forward (2 points)\n",
        "Implement the forward pass for a single timestep of an LSTM in the `lstm_step_forward` function in the file `ECSE552/rnn_layers.py`. This should be similar to the `rnn_step_forward` function that you implemented above, but using the LSTM update rule instead.\n",
        "\n",
        "Once you are done, run the following to perform a simple test of your implementation. You should see errors around `1e-8` or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuyQfb-acvNq",
        "outputId": "0f2e547f-a69a-4ed0-d7c5-7c70925248e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "next_h error:  5.7054131967097955e-09\n",
            "next_c error:  5.8143123088804145e-09\n"
          ]
        }
      ],
      "source": [
        "N, D, H = 3, 4, 5\n",
        "x = np.linspace(-0.4, 1.2, num=N*D).reshape(N, D)\n",
        "prev_h = np.linspace(-0.3, 0.7, num=N*H).reshape(N, H)\n",
        "prev_c = np.linspace(-0.4, 0.9, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-2.1, 1.3, num=4*D*H).reshape(D, 4 * H)\n",
        "Wh = np.linspace(-0.7, 2.2, num=4*H*H).reshape(H, 4 * H)\n",
        "b = np.linspace(0.3, 0.7, num=4*H)\n",
        "\n",
        "next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)\n",
        "\n",
        "expected_next_h = np.asarray([\n",
        "    [ 0.24635157,  0.28610883,  0.32240467,  0.35525807,  0.38474904],\n",
        "    [ 0.49223563,  0.55611431,  0.61507696,  0.66844003,  0.7159181 ],\n",
        "    [ 0.56735664,  0.66310127,  0.74419266,  0.80889665,  0.858299  ]])\n",
        "expected_next_c = np.asarray([\n",
        "    [ 0.32986176,  0.39145139,  0.451556,    0.51014116,  0.56717407],\n",
        "    [ 0.66382255,  0.76674007,  0.87195994,  0.97902709,  1.08751345],\n",
        "    [ 0.74192008,  0.90592151,  1.07717006,  1.25120233,  1.42395676]])\n",
        "\n",
        "print ('next_h error: ', rel_error(expected_next_h, next_h))\n",
        "print ('next_c error: ', rel_error(expected_next_c, next_c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ymcsDJXcvNr"
      },
      "source": [
        "## LSTM: step backward (3 points)\n",
        "Implement the backward pass for a single LSTM timestep in the function `lstm_step_backward` in the file `ECSE552/rnn_layers.py`. Once you are done, run the following to perform numeric gradient checking on your implementation. You should see errors around `1e-8` or less.\n",
        "\n",
        "For this function you need to show the computational graph and explain each step of how you computed the gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adqL0O6VcvNr"
      },
      "source": [
        "Computational Graph: ##### ATTACH IMAGE OF COMPUTATIONAL GRAPH (either directly in markdown or as PDF included in submitted .zip) ######\n",
        "\n",
        "\n",
        "**Explaination with equations:** ##### ADD YOU ANSWER DIRECTLY HERE ######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm7TKdkucvNr",
        "outputId": "4aeb27e9-fa1c-4751-b29e-b4083730fded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dx error:  2.1655622121461696e-10\n",
            "dh error:  6.319540371543225e-10\n",
            "dc error:  5.708393523368808e-10\n",
            "dWx error:  8.51857909674413e-09\n",
            "dWh error:  2.031980079737106e-09\n",
            "db error:  5.174725728721466e-09\n"
          ]
        }
      ],
      "source": [
        "N, D, H = 4, 5, 6\n",
        "x = np.random.randn(N, D)\n",
        "prev_h = np.random.randn(N, H)\n",
        "prev_c = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, 4 * H)\n",
        "Wh = np.random.randn(H, 4 * H)\n",
        "b = np.random.randn(4 * H)\n",
        "\n",
        "next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)\n",
        "\n",
        "dnext_h = np.random.randn(*next_h.shape)\n",
        "dnext_c = np.random.randn(*next_c.shape)\n",
        "\n",
        "fx_h = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
        "fh_h = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
        "fc_h = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
        "fWx_h = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
        "fWh_h = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
        "fb_h = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
        "\n",
        "fx_c = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
        "fh_c = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
        "fc_c = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
        "fWx_c = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
        "fWh_c = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
        "fb_c = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
        "\n",
        "num_grad = eval_numerical_gradient_array\n",
        "\n",
        "dx_num = num_grad(fx_h, x, dnext_h) + num_grad(fx_c, x, dnext_c)\n",
        "dh_num = num_grad(fh_h, prev_h, dnext_h) + num_grad(fh_c, prev_h, dnext_c)\n",
        "dc_num = num_grad(fc_h, prev_c, dnext_h) + num_grad(fc_c, prev_c, dnext_c)\n",
        "dWx_num = num_grad(fWx_h, Wx, dnext_h) + num_grad(fWx_c, Wx, dnext_c)\n",
        "dWh_num = num_grad(fWh_h, Wh, dnext_h) + num_grad(fWh_c, Wh, dnext_c)\n",
        "db_num = num_grad(fb_h, b, dnext_h) + num_grad(fb_c, b, dnext_c)\n",
        "\n",
        "dx, dh, dc, dWx, dWh, db = lstm_step_backward(dnext_h, dnext_c, cache)\n",
        "\n",
        "print ('dx error: ', rel_error(dx_num, dx))\n",
        "print ('dh error: ', rel_error(dh_num, dh))\n",
        "print ('dc error: ', rel_error(dc_num, dc))\n",
        "print ('dWx error: ', rel_error(dWx_num, dWx))\n",
        "print ('dWh error: ', rel_error(dWh_num, dWh))\n",
        "print ('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOS1hMTJcvNr"
      },
      "source": [
        "## LSTM: forward (2 points)\n",
        "In the function `lstm_forward` in the file `ECSE552/rnn_layers.py`, implement the `lstm_forward` function to run an LSTM forward on an entire timeseries of data.\n",
        "\n",
        "When you are done run the following to check your implementation. You should see an error around `1e-7`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxrr-MzAcvNr",
        "outputId": "b0503c12-3f84-4e61-e1fa-2bca4ff24fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h error:  8.610537442272635e-08\n"
          ]
        }
      ],
      "source": [
        "N, D, H, T = 2, 5, 4, 3\n",
        "x = np.linspace(-0.4, 0.6, num=N*T*D).reshape(N, T, D)\n",
        "h0 = np.linspace(-0.4, 0.8, num=N*H).reshape(N, H)\n",
        "Wx = np.linspace(-0.2, 0.9, num=4*D*H).reshape(D, 4 * H)\n",
        "Wh = np.linspace(-0.3, 0.6, num=4*H*H).reshape(H, 4 * H)\n",
        "b = np.linspace(0.2, 0.7, num=4*H)\n",
        "\n",
        "h, cache = lstm_forward(x, h0, Wx, Wh, b)\n",
        "\n",
        "expected_h = np.asarray([\n",
        " [[ 0.01764008,  0.01823233,  0.01882671,  0.0194232 ],\n",
        "  [ 0.11287491,  0.12146228,  0.13018446,  0.13902939],\n",
        "  [ 0.31358768,  0.33338627,  0.35304453,  0.37250975]],\n",
        " [[ 0.45767879,  0.4761092,   0.4936887,   0.51041945],\n",
        "  [ 0.6704845,   0.69350089,  0.71486014,  0.7346449 ],\n",
        "  [ 0.81733511,  0.83677871,  0.85403753,  0.86935314]]])\n",
        "\n",
        "print ('h error: ', rel_error(expected_h, h))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW3Z1tgwcvNu"
      },
      "source": [
        "## LSTM: backward (3 points)\n",
        "Implement the backward pass for an LSTM over an entire timeseries of data in the function `lstm_backward` in the file `ECSE552/rnn_layers.py`. When you are done run the following to perform numeric gradient checking on your implementation. You should see errors around `1e-8` or less.\n",
        "\n",
        "For this function you need to show add the explaination with equations of how you calculated the gradient for the layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tTCOMZrcvNv"
      },
      "source": [
        "**Explaination with equations:** ##### ADD YOU ANSWER DIRECTLY HERE ######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC2O0XVLcvNv",
        "outputId": "6012975f-c128-45aa-cab6-d69a6c47e109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dx error:  7.517689629410135e-10\n",
            "dh0 error:  7.517689629410135e-10\n",
            "dWx error:  7.517689629410135e-10\n",
            "dWh error:  7.517689629410135e-10\n",
            "db error:  7.517689629410135e-10\n"
          ]
        }
      ],
      "source": [
        "N, D, T, H = 2, 3, 10, 6\n",
        "\n",
        "x = np.random.randn(N, T, D)\n",
        "h0 = np.random.randn(N, H)\n",
        "Wx = np.random.randn(D, 4 * H)\n",
        "Wh = np.random.randn(H, 4 * H)\n",
        "b = np.random.randn(4 * H)\n",
        "\n",
        "out, cache = lstm_forward(x, h0, Wx, Wh, b)\n",
        "\n",
        "dout = np.random.randn(*out.shape)\n",
        "\n",
        "dx, dh0, dWx, dWh, db = lstm_backward(dout, cache)\n",
        "\n",
        "fx = lambda x: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
        "fh0 = lambda h0: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWx = lambda Wx: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
        "fWh = lambda Wh: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
        "fb = lambda b: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
        "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
        "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
        "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
        "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
        "\n",
        "print ('dx error: ', rel_error(dx_num, dx))\n",
        "print ('dh0 error: ', rel_error(dx_num, dx))\n",
        "print ('dWx error: ', rel_error(dx_num, dx))\n",
        "print ('dWh error: ', rel_error(dx_num, dx))\n",
        "print ('db error: ', rel_error(dx_num, dx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsShg6kbcvNv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIrPjMGccvNv"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
